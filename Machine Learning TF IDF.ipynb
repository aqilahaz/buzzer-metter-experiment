{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "#TFIDF Vectorizer for Tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataset/full.csv\")\n",
    "del df[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/my_stopwords.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>urls</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tweet</th>\n",
       "      <th>photos</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>link</th>\n",
       "      <th>quote_url</th>\n",
       "      <th>video</th>\n",
       "      <th>new_date</th>\n",
       "      <th>new_time</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>muhyi_goobeh</td>\n",
       "      <td>muhyi_goobeh</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>#WelcomeBackHRS</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/muhyi_goobeh/status/132688...</td>\n",
       "      <td>https://twitter.com/MuhammaD19s/status/1326784...</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-11-12 00:00:00</td>\n",
       "      <td>21:05:57</td>\n",
       "      <td>['welcomebackhrs']</td>\n",
       "      <td>1</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>narasipost</td>\n",
       "      <td>narasipost.com</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://narasipost.com/selamat-datang-sang-h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Berita kepulangan sang Habib ini menjadi sanga...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/narasipost/status/13268707...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-11-12 00:00:00</td>\n",
       "      <td>19:53:40</td>\n",
       "      <td>['welcomebackibhrs', 'welcomebackhrs']</td>\n",
       "      <td>2</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lurahtajir1</td>\n",
       "      <td>Lurah Tajir</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>@sohib212reborn Tagarnya #WelcomeBackIBHRS  #W...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Lurahtajir1/status/1326770...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-11-12 00:00:00</td>\n",
       "      <td>13:16:33</td>\n",
       "      <td>['welcomebackibhrs', 'welcomebackhrs', 'welcom...</td>\n",
       "      <td>3</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trendingviralo</td>\n",
       "      <td>Viralo Trending</td>\n",
       "      <td>0</td>\n",
       "      <td>['https://youtu.be/HKhmkkBre7s']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Kesaksian Habib Rizieq  https://t.co/Ep6R7a4dO...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/TrendingViralo/status/1326...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-11-12 00:00:00</td>\n",
       "      <td>08:36:23</td>\n",
       "      <td>['welcomehomeibhrs', 'welcomebackhrs']</td>\n",
       "      <td>2</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eny86315895</td>\n",
       "      <td>@en_y466</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>#WelcomeBackHRS  #WelcomeHomeIBHRS</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/eny86315895/status/1326671...</td>\n",
       "      <td>https://twitter.com/fadlizon/status/1326038891...</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-11-12 00:00:00</td>\n",
       "      <td>06:41:55</td>\n",
       "      <td>['welcomebackhrs', 'welcomehomeibhrs']</td>\n",
       "      <td>2</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username             name  likes_count  \\\n",
       "0    muhyi_goobeh     muhyi_goobeh            1   \n",
       "1      narasipost   narasipost.com            1   \n",
       "2     lurahtajir1      Lurah Tajir            1   \n",
       "3  trendingviralo  Viralo Trending            0   \n",
       "4     eny86315895         @en_y466            0   \n",
       "\n",
       "                                                urls mentions  \\\n",
       "0                                                 []       []   \n",
       "1  ['https://narasipost.com/selamat-datang-sang-h...       []   \n",
       "2                                                 []       []   \n",
       "3                   ['https://youtu.be/HKhmkkBre7s']       []   \n",
       "4                                                 []       []   \n",
       "\n",
       "                                               tweet photos  replies_count  \\\n",
       "0                                    #WelcomeBackHRS     []              0   \n",
       "1  Berita kepulangan sang Habib ini menjadi sanga...     []              0   \n",
       "2  @sohib212reborn Tagarnya #WelcomeBackIBHRS  #W...     []              0   \n",
       "3  Kesaksian Habib Rizieq  https://t.co/Ep6R7a4dO...     []              0   \n",
       "4                 #WelcomeBackHRS  #WelcomeHomeIBHRS     []              0   \n",
       "\n",
       "   retweets_count                                               link  \\\n",
       "0               0  https://twitter.com/muhyi_goobeh/status/132688...   \n",
       "1               0  https://twitter.com/narasipost/status/13268707...   \n",
       "2               0  https://twitter.com/Lurahtajir1/status/1326770...   \n",
       "3               0  https://twitter.com/TrendingViralo/status/1326...   \n",
       "4               0  https://twitter.com/eny86315895/status/1326671...   \n",
       "\n",
       "                                           quote_url  video  \\\n",
       "0  https://twitter.com/MuhammaD19s/status/1326784...      0   \n",
       "1                                                NaN      0   \n",
       "2                                                NaN      0   \n",
       "3                                                NaN      0   \n",
       "4  https://twitter.com/fadlizon/status/1326038891...      0   \n",
       "\n",
       "              new_date  new_time  \\\n",
       "0  2020-11-12 00:00:00  21:05:57   \n",
       "1  2020-11-12 00:00:00  19:53:40   \n",
       "2  2020-11-12 00:00:00  13:16:33   \n",
       "3  2020-11-12 00:00:00  08:36:23   \n",
       "4  2020-11-12 00:00:00  06:41:55   \n",
       "\n",
       "                                             hashtag  hashtag_count Categories  \n",
       "0                                 ['welcomebackhrs']              1     Buzzer  \n",
       "1             ['welcomebackibhrs', 'welcomebackhrs']              2     Buzzer  \n",
       "2  ['welcomebackibhrs', 'welcomebackhrs', 'welcom...              3     Buzzer  \n",
       "3             ['welcomehomeibhrs', 'welcomebackhrs']              2     Buzzer  \n",
       "4             ['welcomebackhrs', 'welcomehomeibhrs']              2     Buzzer  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing text\n",
    "preprocessed = []\n",
    "for text in df['tweet']:\n",
    "    ## lower all text\n",
    "    temp = text.lower()\n",
    "    ## replace newline with whitespace\n",
    "    temp = re.sub('\\n', \" \", temp)\n",
    "    ## replace - with whitespace\n",
    "    temp = re.sub('-', \" \", temp)\n",
    "    temp = re.sub(\"[^a-zA-Z]\", \" \", str(temp))\n",
    "    temp = word_tokenize(temp)\n",
    "    \n",
    "    temp = [word for word in temp if word not in stopwords]\n",
    "    temp = ' '.join(temp)\n",
    "    preprocessed.append(temp)\n",
    "    \n",
    "df['clean'] = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main', 'ke', 'mall']\n",
      "['main', 'mall']\n",
      "main mall\n"
     ]
    }
   ],
   "source": [
    "## examples\n",
    "text = \"main ke mall\"\n",
    "temp = word_tokenize(text)\n",
    "print(temp)\n",
    "temp=[word for word in temp if word not in stopwords]\n",
    "print(temp)\n",
    "temp = ' '.join(temp)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcomebackhrs</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>berita kepulangan sang habib menjadi sangat tr...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sohib reborn tagarnya welcomebackibhrs welcome...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kesaksian habib rizieq https t co ep r a doj w...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>welcomebackhrs welcomehomeibhrs</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>selamat datang hrs welcomebackhrs</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>romitsut alhamdulillah terimakasih mas perjuan...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>https t co cqeeqfh rl menyambut habib rizieq s...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>trending welcomebackhrs gaada video korea kore...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>tak kata sanggup lontarkan hati terlanjur haru...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 clean Categories\n",
       "0                                       welcomebackhrs     Buzzer\n",
       "1    berita kepulangan sang habib menjadi sangat tr...     Buzzer\n",
       "2    sohib reborn tagarnya welcomebackibhrs welcome...     Buzzer\n",
       "3    kesaksian habib rizieq https t co ep r a doj w...     Buzzer\n",
       "4                      welcomebackhrs welcomehomeibhrs     Buzzer\n",
       "..                                                 ...        ...\n",
       "795                  selamat datang hrs welcomebackhrs        non\n",
       "796  romitsut alhamdulillah terimakasih mas perjuan...        non\n",
       "797  https t co cqeeqfh rl menyambut habib rizieq s...        non\n",
       "798  trending welcomebackhrs gaada video korea kore...        non\n",
       "799  tak kata sanggup lontarkan hati terlanjur haru...        non\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['clean','Categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278    Buzzer\n",
      "54     Buzzer\n",
      "552       non\n",
      "565       non\n",
      "81     Buzzer\n",
      "        ...  \n",
      "53     Buzzer\n",
      "350    Buzzer\n",
      "79     Buzzer\n",
      "792       non\n",
      "520       non\n",
      "Name: Categories, Length: 640, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## separate the features and the classes\n",
    "X = df['clean']\n",
    "y = df['Categories']\n",
    "\n",
    "## splitting dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panjang training data 640\n",
      "panjang test data 160\n"
     ]
    }
   ],
   "source": [
    "print(\"panjang training data\",len(X_train))\n",
    "print(\"panjang test data\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2746)\t1.0\n",
      "  (1, 2749)\t0.8329178960836016\n",
      "  (1, 2748)\t0.473670254251834\n",
      "  (1, 2746)\t0.2861542741611064\n",
      "  (2, 2384)\t0.20879500188404634\n",
      "  (2, 2622)\t0.21462882379162673\n",
      "  (2, 1999)\t0.2562057305217888\n",
      "  (2, 427)\t0.22997362300426416\n",
      "  (2, 1242)\t0.221528770606267\n",
      "  (2, 651)\t0.2037415154867395\n",
      "  (2, 1827)\t0.2562057305217888\n",
      "  (2, 7)\t0.2562057305217888\n",
      "  (2, 6)\t0.221528770606267\n",
      "  (2, 1498)\t0.20879500188404634\n",
      "  (2, 734)\t0.2562057305217888\n",
      "  (2, 1745)\t0.20879500188404634\n",
      "  (2, 2625)\t0.1853675054909177\n",
      "  (2, 2255)\t0.07393541423161447\n",
      "  (2, 234)\t0.2562057305217888\n",
      "  (2, 660)\t0.2562057305217888\n",
      "  (2, 373)\t0.2562057305217888\n",
      "  (2, 1293)\t0.2562057305217888\n",
      "  (2, 34)\t0.2562057305217888\n",
      "  (3, 346)\t0.2477404045490996\n",
      "  (3, 628)\t0.2728693593457698\n",
      "  :\t:\n",
      "  (637, 884)\t0.2177898434080308\n",
      "  (637, 500)\t0.1457654928763051\n",
      "  (637, 963)\t0.1457654928763051\n",
      "  (637, 2746)\t0.2290273918521205\n",
      "  (638, 1269)\t0.4050636700259889\n",
      "  (638, 2310)\t0.36359046128110795\n",
      "  (638, 1622)\t0.36359046128110795\n",
      "  (638, 1218)\t0.36359046128110795\n",
      "  (638, 2660)\t0.38080339812565284\n",
      "  (638, 1173)\t0.35023907020100253\n",
      "  (638, 545)\t0.2978569806358909\n",
      "  (638, 213)\t0.27359670873555486\n",
      "  (638, 2746)\t0.0898708982790606\n",
      "  (639, 2078)\t0.3173059951815101\n",
      "  (639, 702)\t0.3173059951815101\n",
      "  (639, 2736)\t0.3173059951815101\n",
      "  (639, 2362)\t0.3173059951815101\n",
      "  (639, 1601)\t0.3173059951815101\n",
      "  (639, 1856)\t0.25858869638401105\n",
      "  (639, 667)\t0.2848180216900325\n",
      "  (639, 214)\t0.2848180216900325\n",
      "  (639, 1494)\t0.2848180216900325\n",
      "  (639, 1875)\t0.29830174896457273\n",
      "  (639, 1759)\t0.29830174896457273\n",
      "  (639, 2255)\t0.09156762475273544\n",
      "  (0, 2255)\t0.3256972609419285\n",
      "  (0, 1150)\t0.9454741108115681\n",
      "  (1, 2746)\t0.1786387897499688\n",
      "  (1, 1538)\t0.8051558977460767\n",
      "  (1, 73)\t0.5655193746648967\n",
      "  (2, 2595)\t0.623528629305606\n",
      "  (2, 2255)\t0.17993683201032132\n",
      "  (2, 1831)\t0.48499810849341773\n",
      "  (2, 336)\t0.5861839469655993\n",
      "  (3, 2746)\t0.1338360627423234\n",
      "  (3, 2337)\t0.4298342675475398\n",
      "  (3, 2334)\t0.4513227242211167\n",
      "  (3, 83)\t0.5670937807451452\n",
      "  (3, 0)\t0.5215772744218348\n",
      "  (4, 2747)\t0.5184188940423794\n",
      "  (4, 2746)\t0.2389801056617121\n",
      "  (4, 1308)\t0.8210544192669529\n",
      "  (5, 2255)\t0.4748562795820054\n",
      "  (5, 1774)\t0.6705015371887979\n",
      "  (5, 505)\t0.5700343870057273\n",
      "  (6, 2262)\t0.41200937352758565\n",
      "  (6, 2255)\t0.13750847628370422\n",
      "  (6, 1618)\t0.44796421315555707\n",
      "  (6, 469)\t0.4765031749039707\n",
      "  (6, 60)\t0.44796421315555707\n",
      "  :\t:\n",
      "  (155, 1601)\t0.2139772329711209\n",
      "  (155, 1499)\t0.5550474604289763\n",
      "  (155, 1178)\t0.17925314058060404\n",
      "  (155, 1157)\t0.3841375398323591\n",
      "  (155, 1003)\t0.20116160363554533\n",
      "  (155, 912)\t0.14119889403310948\n",
      "  (156, 2747)\t0.24258434546621813\n",
      "  (156, 2746)\t0.11182623391549258\n",
      "  (156, 2577)\t0.40081004997281344\n",
      "  (156, 2301)\t0.4524151060158739\n",
      "  (156, 2148)\t0.4358019886373759\n",
      "  (156, 904)\t0.4524151060158739\n",
      "  (156, 570)\t0.41075151001645055\n",
      "  (157, 2255)\t0.16983153622037273\n",
      "  (157, 1874)\t0.5282552899835992\n",
      "  (157, 1479)\t0.47960745596175575\n",
      "  (157, 740)\t0.4930078939913136\n",
      "  (157, 273)\t0.4679994685440509\n",
      "  (158, 2255)\t0.2772642159427281\n",
      "  (158, 870)\t0.9607937107192491\n",
      "  (159, 2747)\t0.35840881115739404\n",
      "  (159, 2746)\t0.1652188539900723\n",
      "  (159, 1537)\t0.643880265016118\n",
      "  (159, 73)\t0.5230356918678848\n",
      "  (159, 27)\t0.3950920446250562\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "## vectorizing the training features\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "print(X_train_vectorized)\n",
    "\n",
    "## vectorizing the testing features\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "print(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278    Buzzer\n",
       "54     Buzzer\n",
       "552       non\n",
       "565       non\n",
       "81     Buzzer\n",
       "        ...  \n",
       "53     Buzzer\n",
       "350    Buzzer\n",
       "79     Buzzer\n",
       "792       non\n",
       "520       non\n",
       "Name: Categories, Length: 640, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer'\n",
      " 'non' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer'\n",
      " 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'non' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'non' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'non' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'non' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non'\n",
      " 'non' 'non' 'non' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'Buzzer' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'Buzzer']\n",
      "[[82  4]\n",
      " [ 8 66]]\n",
      "0.925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Buzzer       0.91      0.95      0.93        86\n",
      "         non       0.94      0.89      0.92        74\n",
      "\n",
      "    accuracy                           0.93       160\n",
      "   macro avg       0.93      0.92      0.92       160\n",
      "weighted avg       0.93      0.93      0.92       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification using decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "## predicting the testing data\n",
    "y_pred = clf.predict(X_test_vectorized)\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "### result of decision tree\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "print(conf)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "\n",
    "rep = classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Vectorizer and Decision Tree Classifier\n",
    "import pickle\n",
    "\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pickle\", \"wb\"))\n",
    "filename = 'finalized_model_word.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open vectorizer and classifier\n",
    "load_vectorizer = pickle.load(open(\"vectorizer.pickle\", \"rb\"))\n",
    "clf = pickle.load(open(\"finalized_model_word.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read unlabeled dataset for test\n",
    "\n",
    "data=pd.read_csv(\"tidak.csv\")\n",
    "# data=data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read stopword source\n",
    "with open('dataset/my_stopwords.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing text for cleaning\n",
    "preprocessed = []\n",
    "for text in data['tweet']:\n",
    "    ## lower all text\n",
    "    temp = text.lower()\n",
    "    ## replace newline with whitespace\n",
    "    temp = re.sub('\\n', \" \", temp)\n",
    "    ## replace - with whitespace\n",
    "    temp = re.sub('-', \" \", temp)\n",
    "    ## replace all non alphabetic characters with whitespace\n",
    "    temp = re.sub(\"[^a-zA-Z]\", \" \", str(temp))\n",
    "    temp = word_tokenize(temp)\n",
    "    \n",
    "    temp = [word for word in temp if word not in stopwords]\n",
    "    temp = ' '.join(temp)\n",
    "    preprocessed.append(temp)\n",
    "    \n",
    "data['clean'] = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>urls</th>\n",
       "      <th>...</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>link</th>\n",
       "      <th>retweet</th>\n",
       "      <th>quote_url</th>\n",
       "      <th>video</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>source</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1327038460184514560</td>\n",
       "      <td>revelieolus</td>\n",
       "      <td>icam.</td>\n",
       "      <td>995523325307703296</td>\n",
       "      <td>2020-11-13 06:59:58 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1327038460184514560</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/revelieolus/status/1327038...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bisanya temen presentasi typo ter kon sebagian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1327038459991658496</td>\n",
       "      <td>hyppechan</td>\n",
       "      <td>𝐇𝐚𝐞𝐜𝐚. #90sLove</td>\n",
       "      <td>1250955639108464642</td>\n",
       "      <td>2020-11-13 06:59:58 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1327036806592753665</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/hyppechan/status/132703845...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>holmestowns mrs shampo pantine berkata quiz ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1327038458120921088</td>\n",
       "      <td>ikuchanx3_</td>\n",
       "      <td>X3🎥📽〽</td>\n",
       "      <td>746891414</td>\n",
       "      <td>2020-11-13 06:59:58 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1326123159574114305</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/ikuchanX3_/status/13270384...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jntexpressid besok checkout pakai jnt alasan p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1327038447568097280</td>\n",
       "      <td>peluangusahaoke</td>\n",
       "      <td>Usaha Cemilan Unik</td>\n",
       "      <td>3836205673</td>\n",
       "      <td>2020-11-13 06:59:55 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>['peluangusaha', 'peluangusahaonline']</td>\n",
       "      <td>1327038447568097280</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/peluangusahaoke/status/132...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cinta selalu melekat kebersamaan melekat doa d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1327038444170665984</td>\n",
       "      <td>aviranie</td>\n",
       "      <td>anakditengah</td>\n",
       "      <td>288598180</td>\n",
       "      <td>2020-11-13 06:59:54 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1327038444170665984</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/aviranie/status/1327038444...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/BBCIndonesia/status/132679...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tuhan maafkan menjaga titipan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>1327019632927019008</td>\n",
       "      <td>amimia04568308</td>\n",
       "      <td>Amimia</td>\n",
       "      <td>1312347666974531584</td>\n",
       "      <td>2020-11-13 05:45:09 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>['7daysbeforebe']</td>\n",
       "      <td>1327019632927019008</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Amimia04568308/status/1327...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kabar jantung sekarang baik segerombolan orang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>1327019624215375875</td>\n",
       "      <td>animmnfh17</td>\n",
       "      <td>Anim Munifah</td>\n",
       "      <td>1281780877723230208</td>\n",
       "      <td>2020-11-13 05:45:07 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>['oligarkilahirkanomnibuslaw', 'omnibuslawtida...</td>\n",
       "      <td>1327019624215375875</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/animmnfh17/status/13270196...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>siapa memperbanyak istighfar allah membebaskan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>1327019615524798464</td>\n",
       "      <td>winarnibtl</td>\n",
       "      <td>Win</td>\n",
       "      <td>1205825224983187457</td>\n",
       "      <td>2020-11-13 05:45:05 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>['oligarkilahirkanomnibuslaw', 'omnibuslawtida...</td>\n",
       "      <td>1327019615524798464</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Winarnibtl/status/13270196...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>seorang pemimpin kaum muslimin bersungguh sung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>1327019613549334528</td>\n",
       "      <td>genta__kojima</td>\n",
       "      <td>Genta Ndut</td>\n",
       "      <td>531052492</td>\n",
       "      <td>2020-11-13 05:45:05 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1327019613549334528</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Genta__Kojima/status/13270...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alasan membunuh orang banyak diperlukan alasan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>1327019609732521984</td>\n",
       "      <td>teekuz</td>\n",
       "      <td>𝘈𝘴𝘦𝘱𝙇𝙚𝙢𝙗𝙤𝙠</td>\n",
       "      <td>60284668</td>\n",
       "      <td>2020-11-13 05:45:04 SE Asia Standard Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>1327019609732521984</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/teekuz/status/132701960973...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kalian selalu asyik masyuk kata kasar diajarka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3096 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id         username                name  \\\n",
       "0     1327038460184514560      revelieolus               icam.   \n",
       "1     1327038459991658496        hyppechan     𝐇𝐚𝐞𝐜𝐚. #90sLove   \n",
       "2     1327038458120921088       ikuchanx3_               X3🎥📽〽   \n",
       "3     1327038447568097280  peluangusahaoke  Usaha Cemilan Unik   \n",
       "4     1327038444170665984         aviranie        anakditengah   \n",
       "...                   ...              ...                 ...   \n",
       "3091  1327019632927019008   amimia04568308              Amimia   \n",
       "3092  1327019624215375875       animmnfh17        Anim Munifah   \n",
       "3093  1327019615524798464       winarnibtl                 Win   \n",
       "3094  1327019613549334528    genta__kojima          Genta Ndut   \n",
       "3095  1327019609732521984           teekuz          𝘈𝘴𝘦𝘱𝙇𝙚𝙢𝙗𝙤𝙠   \n",
       "\n",
       "                  user_id                                 created_at  geo  \\\n",
       "0      995523325307703296  2020-11-13 06:59:58 SE Asia Standard Time  NaN   \n",
       "1     1250955639108464642  2020-11-13 06:59:58 SE Asia Standard Time  NaN   \n",
       "2               746891414  2020-11-13 06:59:58 SE Asia Standard Time  NaN   \n",
       "3              3836205673  2020-11-13 06:59:55 SE Asia Standard Time  NaN   \n",
       "4               288598180  2020-11-13 06:59:54 SE Asia Standard Time  NaN   \n",
       "...                   ...                                        ...  ...   \n",
       "3091  1312347666974531584  2020-11-13 05:45:09 SE Asia Standard Time  NaN   \n",
       "3092  1281780877723230208  2020-11-13 05:45:07 SE Asia Standard Time  NaN   \n",
       "3093  1205825224983187457  2020-11-13 05:45:05 SE Asia Standard Time  NaN   \n",
       "3094            531052492  2020-11-13 05:45:05 SE Asia Standard Time  NaN   \n",
       "3095             60284668  2020-11-13 05:45:04 SE Asia Standard Time  NaN   \n",
       "\n",
       "      likes_count                                           hashtags  \\\n",
       "0               0                                                 []   \n",
       "1               0                                                 []   \n",
       "2               0                                                 []   \n",
       "3               0             ['peluangusaha', 'peluangusahaonline']   \n",
       "4               0                                                 []   \n",
       "...           ...                                                ...   \n",
       "3091            2                                  ['7daysbeforebe']   \n",
       "3092            0  ['oligarkilahirkanomnibuslaw', 'omnibuslawtida...   \n",
       "3093            0  ['oligarkilahirkanomnibuslaw', 'omnibuslawtida...   \n",
       "3094            0                                                 []   \n",
       "3095           11                                                 []   \n",
       "\n",
       "          conversation_id urls  ... replies_count retweets_count  \\\n",
       "0     1327038460184514560   []  ...             0              0   \n",
       "1     1327036806592753665   []  ...             1              0   \n",
       "2     1326123159574114305   []  ...             0              0   \n",
       "3     1327038447568097280   []  ...             0              0   \n",
       "4     1327038444170665984   []  ...             0              0   \n",
       "...                   ...  ...  ...           ...            ...   \n",
       "3091  1327019632927019008   []  ...             0              0   \n",
       "3092  1327019624215375875   []  ...             0              0   \n",
       "3093  1327019615524798464   []  ...             0              0   \n",
       "3094  1327019613549334528   []  ...             0              0   \n",
       "3095  1327019609732521984   []  ...             2              1   \n",
       "\n",
       "                                                   link retweet  \\\n",
       "0     https://twitter.com/revelieolus/status/1327038...   False   \n",
       "1     https://twitter.com/hyppechan/status/132703845...   False   \n",
       "2     https://twitter.com/ikuchanX3_/status/13270384...   False   \n",
       "3     https://twitter.com/peluangusahaoke/status/132...   False   \n",
       "4     https://twitter.com/aviranie/status/1327038444...   False   \n",
       "...                                                 ...     ...   \n",
       "3091  https://twitter.com/Amimia04568308/status/1327...   False   \n",
       "3092  https://twitter.com/animmnfh17/status/13270196...   False   \n",
       "3093  https://twitter.com/Winarnibtl/status/13270196...   False   \n",
       "3094  https://twitter.com/Genta__Kojima/status/13270...   False   \n",
       "3095  https://twitter.com/teekuz/status/132701960973...   False   \n",
       "\n",
       "                                              quote_url  video  user_rt_id  \\\n",
       "0                                                   NaN      0         NaN   \n",
       "1                                                   NaN      0         NaN   \n",
       "2                                                   NaN      0         NaN   \n",
       "3                                                   NaN      0         NaN   \n",
       "4     https://twitter.com/BBCIndonesia/status/132679...      0         NaN   \n",
       "...                                                 ...    ...         ...   \n",
       "3091                                                NaN      1         NaN   \n",
       "3092                                                NaN      0         NaN   \n",
       "3093                                                NaN      0         NaN   \n",
       "3094                                                NaN      0         NaN   \n",
       "3095                                                NaN      0         NaN   \n",
       "\n",
       "     source  retweet_date                                              clean  \n",
       "0       NaN           NaN  bisanya temen presentasi typo ter kon sebagian...  \n",
       "1       NaN           NaN  holmestowns mrs shampo pantine berkata quiz ea...  \n",
       "2       NaN           NaN  jntexpressid besok checkout pakai jnt alasan p...  \n",
       "3       NaN           NaN  cinta selalu melekat kebersamaan melekat doa d...  \n",
       "4       NaN           NaN                      tuhan maafkan menjaga titipan  \n",
       "...     ...           ...                                                ...  \n",
       "3091    NaN           NaN  kabar jantung sekarang baik segerombolan orang...  \n",
       "3092    NaN           NaN  siapa memperbanyak istighfar allah membebaskan...  \n",
       "3093    NaN           NaN  seorang pemimpin kaum muslimin bersungguh sung...  \n",
       "3094    NaN           NaN  alasan membunuh orang banyak diperlukan alasan...  \n",
       "3095    NaN           NaN  kalian selalu asyik masyuk kata kasar diajarka...  \n",
       "\n",
       "[3096 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the feature of interest\n",
    "X = data['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2531)\t0.7826881093752713\n",
      "  (0, 2524)\t0.6224141092894371\n",
      "  (1, 303)\t1.0\n",
      "  (2, 2316)\t0.1557232591932563\n",
      "  (2, 2040)\t0.1990886570507602\n",
      "  (2, 2036)\t0.1774059581220083\n",
      "  (2, 1939)\t0.36621850790321114\n",
      "  (2, 1902)\t0.3801790478195099\n",
      "  (2, 1119)\t0.21177222283850689\n",
      "  (2, 1118)\t0.6353166685155207\n",
      "  (2, 476)\t0.21177222283850689\n",
      "  (2, 457)\t0.21177222283850689\n",
      "  (2, 336)\t0.1990886570507602\n",
      "  (2, 185)\t0.1614265550228536\n",
      "  (2, 67)\t0.14874298923510695\n",
      "  (3, 2316)\t0.5924111658868414\n",
      "  (3, 490)\t0.8056357803204829\n",
      "  (4, 2626)\t0.6849515931211451\n",
      "  (4, 1468)\t0.7285885773746423\n",
      "  (5, 2730)\t0.2705536159482251\n",
      "  (5, 2702)\t0.24883823872274968\n",
      "  (5, 2617)\t0.24883823872274968\n",
      "  (5, 2312)\t0.2238512205259084\n",
      "  (5, 2254)\t0.24883823872274968\n",
      "  (5, 1928)\t0.2345346854078983\n",
      "  :\t:\n",
      "  (3092, 73)\t0.2139473598524288\n",
      "  (3092, 8)\t0.28636302639615396\n",
      "  (3093, 2794)\t0.39475682972815945\n",
      "  (3093, 2466)\t0.39475682972815945\n",
      "  (3093, 2349)\t0.3543389062897702\n",
      "  (3093, 1993)\t0.3543389062897702\n",
      "  (3093, 1732)\t0.3543389062897702\n",
      "  (3093, 1188)\t0.39475682972815945\n",
      "  (3093, 300)\t0.39475682972815945\n",
      "  (3094, 2363)\t0.7306323373271901\n",
      "  (3094, 1909)\t0.47184204285661885\n",
      "  (3094, 213)\t0.49349921402642544\n",
      "  (3095, 2483)\t0.250622016514435\n",
      "  (3095, 2316)\t0.18429082300833957\n",
      "  (3095, 2266)\t0.2356116398384408\n",
      "  (3095, 2159)\t0.250622016514435\n",
      "  (3095, 2020)\t0.2356116398384408\n",
      "  (3095, 1530)\t0.2356116398384408\n",
      "  (3095, 1466)\t0.433401603408591\n",
      "  (3095, 1407)\t0.250622016514435\n",
      "  (3095, 1182)\t0.2356116398384408\n",
      "  (3095, 1175)\t0.250622016514435\n",
      "  (3095, 1157)\t0.4499232161987688\n",
      "  (3095, 271)\t0.2356116398384408\n",
      "  (3095, 184)\t0.2167008017042955\n"
     ]
    }
   ],
   "source": [
    "## vectorizing the training features using existing vectorizer\n",
    "X_train_vectorized = load_vectorizer.transform(X)\n",
    "print(X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non' 'non' 'non' ... 'non' 'non' 'non']\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train_vectorized)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non       3078\n",
       "Buzzer      18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
