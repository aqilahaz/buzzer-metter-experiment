{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "#TFIDF Vectorizer for Tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"DataFrame.csv\")\n",
    "df=pd.read_csv(\"dataset/new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buzzer    1014\n",
       "non        984\n",
       "Name: Categories, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Categories'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username           0\n",
       "name               0\n",
       "user_id            0\n",
       "created_at         0\n",
       "likes_count        0\n",
       "hashtags           0\n",
       "conversation_id    0\n",
       "urls               0\n",
       "mentions           0\n",
       "Categories         0\n",
       "tweet              0\n",
       "photos             0\n",
       "replies_count      0\n",
       "retweets_count     0\n",
       "link               0\n",
       "retweet            0\n",
       "video              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Categories'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/my_stopwords.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing text\n",
    "preprocessed = []\n",
    "for text in df['tweet']:\n",
    "    ## lower all text\n",
    "    temp = text.lower()\n",
    "    ## replace newline with whitespace\n",
    "    temp = re.sub('\\n', \" \", temp)\n",
    "    ## replace - with whitespace\n",
    "    temp = re.sub('-', \" \", temp)\n",
    "    temp = re.sub(\"[^a-zA-Z]\", \" \", str(temp))\n",
    "    temp = word_tokenize(temp)\n",
    "    \n",
    "    temp = [word for word in temp if word not in stopwords]\n",
    "    temp = ' '.join(temp)\n",
    "    preprocessed.append(temp)\n",
    "    \n",
    "df['clean'] = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beliau memang imam besar umat islam pengaruhny...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kamipercayahrs https t co f e owov https t co ...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dahnilanzar ane naik bus jakarta dibawa hutan ...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semoga pengikut dajjal kaum firaun tlah menhin...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heh nihhh pereman pereman murahan menghina ibh...</td>\n",
       "      <td>Buzzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>dibawah nikmati prosesnya diatas cuma turun ke...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>berbohong seseorang melihat arah wajah orang t...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>fakta libra seseorang besar kepala berani menc...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>berupaya menuju kesejahteraan hidup lebih baik...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>punyahan kapan kapan pernah okesippp</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1998 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  clean Categories\n",
       "0     beliau memang imam besar umat islam pengaruhny...     Buzzer\n",
       "1     kamipercayahrs https t co f e owov https t co ...     Buzzer\n",
       "2     dahnilanzar ane naik bus jakarta dibawa hutan ...        non\n",
       "3     semoga pengikut dajjal kaum firaun tlah menhin...     Buzzer\n",
       "4     heh nihhh pereman pereman murahan menghina ibh...     Buzzer\n",
       "...                                                 ...        ...\n",
       "1993  dibawah nikmati prosesnya diatas cuma turun ke...        non\n",
       "1994  berbohong seseorang melihat arah wajah orang t...        non\n",
       "1995  fakta libra seseorang besar kepala berani menc...        non\n",
       "1996  berupaya menuju kesejahteraan hidup lebih baik...        non\n",
       "1997               punyahan kapan kapan pernah okesippp        non\n",
       "\n",
       "[1998 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['clean','Categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979     Buzzer\n",
      "1431       non\n",
      "864     Buzzer\n",
      "1087       non\n",
      "1064       non\n",
      "         ...  \n",
      "1879       non\n",
      "1895       non\n",
      "1859       non\n",
      "792     Buzzer\n",
      "1544    Buzzer\n",
      "Name: Categories, Length: 1598, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## separate the features and the classes\n",
    "X = df['clean']\n",
    "y = df['Categories']\n",
    "\n",
    "## splitting dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panjang training data 1598\n",
      "panjang test data 400\n"
     ]
    }
   ],
   "source": [
    "print(\"panjang training data\",len(X_train))\n",
    "print(\"panjang test data\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3321)\t0.45167504552114174\n",
      "  (0, 2645)\t0.45167504552114174\n",
      "  (0, 875)\t0.1544621281501226\n",
      "  (0, 1745)\t0.1544621281501226\n",
      "  (0, 2092)\t0.22290862291927863\n",
      "  (0, 2103)\t0.0989766152417148\n",
      "  (0, 2999)\t0.45167504552114174\n",
      "  (0, 4504)\t0.3416480003268816\n",
      "  (0, 1889)\t0.29249221864880015\n",
      "  (0, 5074)\t0.280164564249592\n",
      "  (1, 1702)\t0.22157478170856407\n",
      "  (1, 4604)\t0.22157478170856407\n",
      "  (1, 5272)\t0.22157478170856407\n",
      "  (1, 1557)\t0.2098828243577593\n",
      "  (1, 4895)\t0.16990775813935857\n",
      "  (1, 5100)\t0.2098828243577593\n",
      "  (1, 2206)\t0.22157478170856407\n",
      "  (1, 696)\t0.22157478170856407\n",
      "  (1, 3962)\t0.22157478170856407\n",
      "  (1, 4358)\t0.1476121203236189\n",
      "  (1, 937)\t0.22157478170856407\n",
      "  (1, 4908)\t0.18989529124855892\n",
      "  (1, 4372)\t0.09010820523479053\n",
      "  (1, 749)\t0.22157478170856407\n",
      "  (1, 1193)\t0.18989529124855892\n",
      "  :\t:\n",
      "  (1596, 4944)\t0.27670603161222473\n",
      "  (1596, 3998)\t0.332180217936935\n",
      "  (1596, 875)\t0.15908900267969367\n",
      "  (1596, 1745)\t0.15908900267969367\n",
      "  (1596, 2103)\t0.611648610413258\n",
      "  (1597, 2819)\t0.16594663895709647\n",
      "  (1597, 542)\t0.16594663895709647\n",
      "  (1597, 3536)\t0.33189327791419293\n",
      "  (1597, 483)\t0.16594663895709647\n",
      "  (1597, 3850)\t0.1656413908826334\n",
      "  (1597, 4183)\t0.1638474287031564\n",
      "  (1597, 3837)\t0.15351525070159547\n",
      "  (1597, 3737)\t0.16594663895709647\n",
      "  (1597, 124)\t0.1650363402650713\n",
      "  (1597, 829)\t0.1647364948430189\n",
      "  (1597, 3749)\t0.3300726805301426\n",
      "  (1597, 114)\t0.16443840805525514\n",
      "  (1597, 2002)\t0.4978399168712894\n",
      "  (1597, 3590)\t0.3288768161105103\n",
      "  (1597, 845)\t0.16594663895709647\n",
      "  (1597, 629)\t0.1656413908826334\n",
      "  (1597, 2003)\t0.1656413908826334\n",
      "  (1597, 3797)\t0.15673144506779468\n",
      "  (1597, 335)\t0.152565248245334\n",
      "  (1597, 4372)\t0.15881681047807003\n",
      "  (0, 4372)\t0.15881681047807003\n",
      "  (0, 4183)\t0.1638474287031564\n",
      "  (0, 3850)\t0.1656413908826334\n",
      "  (0, 3837)\t0.15351525070159547\n",
      "  (0, 3797)\t0.15673144506779468\n",
      "  (0, 3749)\t0.3300726805301426\n",
      "  (0, 3737)\t0.16594663895709647\n",
      "  (0, 3590)\t0.3288768161105103\n",
      "  (0, 3536)\t0.33189327791419293\n",
      "  (0, 2819)\t0.16594663895709647\n",
      "  (0, 2003)\t0.1656413908826334\n",
      "  (0, 2002)\t0.4978399168712894\n",
      "  (0, 845)\t0.16594663895709647\n",
      "  (0, 829)\t0.1647364948430189\n",
      "  (0, 629)\t0.1656413908826334\n",
      "  (0, 542)\t0.16594663895709647\n",
      "  (0, 483)\t0.16594663895709647\n",
      "  (0, 335)\t0.152565248245334\n",
      "  (0, 124)\t0.1650363402650713\n",
      "  (0, 114)\t0.16443840805525514\n",
      "  (1, 4372)\t0.15881681047807003\n",
      "  (1, 4183)\t0.1638474287031564\n",
      "  (1, 3850)\t0.1656413908826334\n",
      "  (1, 3837)\t0.15351525070159547\n",
      "  (1, 3797)\t0.15673144506779468\n",
      "  :\t:\n",
      "  (395, 4010)\t0.5701289712170359\n",
      "  (395, 2103)\t0.14577590474289198\n",
      "  (395, 1889)\t0.43079183602769056\n",
      "  (395, 596)\t0.5567833715212168\n",
      "  (396, 4406)\t0.28000785501570896\n",
      "  (396, 4372)\t0.19050643999428873\n",
      "  (396, 3275)\t0.37675684081727934\n",
      "  (396, 2929)\t0.7678749168704854\n",
      "  (396, 2407)\t0.3920781712729396\n",
      "  (397, 3221)\t0.4663660280684362\n",
      "  (397, 2407)\t0.39033178630155146\n",
      "  (397, 1590)\t0.4663660280684362\n",
      "  (397, 1490)\t0.4663660280684362\n",
      "  (397, 960)\t0.4417570375145729\n",
      "  (398, 2750)\t0.3731137260803379\n",
      "  (398, 2680)\t0.7462274521606758\n",
      "  (398, 1745)\t0.13470402526275013\n",
      "  (398, 1448)\t0.3731137260803379\n",
      "  (398, 875)\t0.13470402526275013\n",
      "  (398, 78)\t0.3583664822757627\n",
      "  (399, 3551)\t0.2543232453358682\n",
      "  (399, 3174)\t0.43454763372167676\n",
      "  (399, 2938)\t0.48237554574837355\n",
      "  (399, 1659)\t0.48237554574837355\n",
      "  (399, 660)\t0.5302034577750703\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "## vectorizing the training features\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "print(X_train_vectorized)\n",
    "\n",
    "## vectorizing the testing features\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "print(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buzzer' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non'\n",
      " 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'non' 'non' 'non'\n",
      " 'non' 'Buzzer' 'non' 'non' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer'\n",
      " 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non'\n",
      " 'non' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non' 'non'\n",
      " 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non'\n",
      " 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'non' 'non' 'Buzzer' 'non' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer'\n",
      " 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non'\n",
      " 'non' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'non' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'non'\n",
      " 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'non' 'non' 'Buzzer'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer'\n",
      " 'non' 'non' 'non' 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer'\n",
      " 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'non'\n",
      " 'non' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'non'\n",
      " 'non' 'non' 'non' 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'non' 'non'\n",
      " 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non'\n",
      " 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'non'\n",
      " 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non'\n",
      " 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'non' 'Buzzer' 'non' 'non'\n",
      " 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'non' 'non' 'non'\n",
      " 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'non' 'non' 'non' 'non' 'Buzzer'\n",
      " 'Buzzer' 'non' 'non' 'Buzzer' 'Buzzer' 'non' 'non' 'Buzzer' 'non' 'non'\n",
      " 'non' 'non' 'non' 'non' 'non' 'non' 'non' 'non' 'non' 'Buzzer' 'non'\n",
      " 'non' 'Buzzer' 'Buzzer' 'Buzzer' 'non' 'Buzzer' 'non' 'Buzzer' 'Buzzer'\n",
      " 'Buzzer' 'non' 'non' 'non' 'Buzzer' 'non' 'Buzzer' 'non' 'non' 'non'\n",
      " 'non']\n",
      "[[170  24]\n",
      " [ 27 179]]\n",
      "0.8725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Buzzer       0.86      0.88      0.87       194\n",
      "         non       0.88      0.87      0.88       206\n",
      "\n",
      "    accuracy                           0.87       400\n",
      "   macro avg       0.87      0.87      0.87       400\n",
      "weighted avg       0.87      0.87      0.87       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification using decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "dtc.fit(X_train_vectorized, y_train)\n",
    "## predicting the testing data\n",
    "y_pred = dtc.predict(X_test_vectorized)\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "### result of decision tree\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "print(conf)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "\n",
    "rep = classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buzzer    1014\n",
       "non        984\n",
       "Name: Categories, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Categories'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Vectorizer and Decision Tree Classifier\n",
    "import pickle\n",
    "\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pickle\", \"wb\"))\n",
    "filename = 'dtc_word.sav'\n",
    "pickle.dump(dtc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Label New Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "#TFIDF Vectorizer for Tweet\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"bukan.csv\")\n",
    "# data=data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/my_stopwords.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-a019642e7cb1>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean'] = preprocessed\n"
     ]
    }
   ],
   "source": [
    "## preprocessing text\n",
    "import re\n",
    "preprocessed = []\n",
    "for text in data['tweet']:\n",
    "    ## lower all text\n",
    "    temp = text.lower()\n",
    "    ## replace newline with whitespace\n",
    "    temp = re.sub('\\n', \" \", temp)\n",
    "    ## replace - with whitespace\n",
    "    temp = re.sub('-', \" \", temp)\n",
    "    temp = re.sub(\"[^a-zA-Z]\", \" \", str(temp))\n",
    "    temp = word_tokenize(temp)\n",
    "    \n",
    "    temp = [word for word in temp if word not in stopwords]\n",
    "    temp = ' '.join(temp)\n",
    "    preprocessed.append(temp)\n",
    "    \n",
    "data['clean'] = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load(\"dtc_word.sav\")\n",
    "vektor = joblib.load('vectorizer.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4252)\t0.7499472175067238\n",
      "  (0, 4251)\t0.49592804667212526\n",
      "  (0, 740)\t0.4377608290812385\n",
      "  (1, 5361)\t0.3220711456695676\n",
      "  (1, 5349)\t0.3326904319584545\n",
      "  (1, 4727)\t0.3060586058139504\n",
      "  (1, 2777)\t0.36567694807039064\n",
      "  (1, 2748)\t0.3326904319584545\n",
      "  (1, 2612)\t0.3463810731154737\n",
      "  (1, 1745)\t0.12505282320661343\n",
      "  (1, 1233)\t0.3220711456695676\n",
      "  (1, 875)\t0.12505282320661343\n",
      "  (1, 740)\t0.21345374744557405\n",
      "  (1, 161)\t0.2207896986351613\n",
      "  (1, 152)\t0.31339455700353763\n",
      "  (2, 4400)\t0.2585864173731646\n",
      "  (2, 4343)\t0.29136812509403714\n",
      "  (2, 4032)\t0.38520263850320596\n",
      "  (2, 3911)\t0.38520263850320596\n",
      "  (2, 3238)\t0.35045477400587816\n",
      "  (2, 1745)\t0.13173014516132384\n",
      "  (2, 875)\t0.13173014516132384\n",
      "  (2, 740)\t0.22485132614540596\n",
      "  (2, 378)\t0.489337159339898\n",
      "  (2, 356)\t0.3224009145729699\n",
      "  :\t:\n",
      "  (1295, 2738)\t0.5307230995547229\n",
      "  (1295, 2012)\t0.4801813481911826\n",
      "  (1295, 740)\t0.3270526112670136\n",
      "  (1296, 740)\t1.0\n",
      "  (1297, 5003)\t0.25134504707005584\n",
      "  (1297, 3257)\t0.3003055231227787\n",
      "  (1297, 2121)\t0.8196478454638193\n",
      "  (1297, 1665)\t0.2844591378009109\n",
      "  (1297, 740)\t0.17529499638249407\n",
      "  (1297, 313)\t0.25134504707005584\n",
      "  (1298, 4406)\t0.7154398691318311\n",
      "  (1298, 740)\t0.6986743115763084\n",
      "  (1299, 5046)\t0.26769324044625664\n",
      "  (1299, 4866)\t0.28462963336936553\n",
      "  (1299, 4648)\t0.32197885082047356\n",
      "  (1299, 4550)\t0.35390330529837855\n",
      "  (1299, 4051)\t0.23945533313910955\n",
      "  (1299, 3831)\t0.25021499124791363\n",
      "  (1299, 3590)\t0.14901686484099563\n",
      "  (1299, 2707)\t0.264280030885828\n",
      "  (1299, 2610)\t0.32197885082047356\n",
      "  (1299, 2192)\t0.31170147201008325\n",
      "  (1299, 772)\t0.3352286965728245\n",
      "  (1299, 740)\t0.2065812109511825\n",
      "  (1299, 378)\t0.22478822934421297\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vektor.transform(X)\n",
    "print(X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non' 'non' 'non' ... 'non' 'non' 'non']\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(X_train_vectorized)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non       1294\n",
       "Buzzer       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
